---
title: "페이스북 잠재의미분석"
output: html_notebook
---

1. Environment Setting & Packages loading
```{r}
setwd("C:\\Users\\kyuchul\\Documents\\Carrer\\Rzip")
packages <- c("Rfacebook","tm","lsa","wordcloud","ggplot2","KoNLP","GPArotation","cluster","RWeka","ROAuth","fpc","stringr","ape")
for(i in packages){
  library(i,character.only = T)
}
```

* 어떤 dictionary를 사용할 것인가 
```{r}
useSejongDic()
```
사전에 별도의 단어를 추가하기 위해서는 mergeUserDic(data.frame(c("item1","item2.."),c("품사")))

* Setting
```{r}
pdf.options(family="Korea1deb") #not to tear down the letters
options(java.parameters=c("-Xmx6g","-Dfile.encoding=UTF-8")) #to increse heap size of rjava
pal <- brewer.pal(9,"Set1")
options(mc.cores=1)
```

2. 대나무숲 파일 로딩
```{r}

path1 <- "C:\\Users\\kyuchul\\Documents\\Carrer\\Rzip\\data_"
path2 <- c("CAU","CAUdark","hanyang","KU","kyunghee","skku","skkudark","snu","union","yonsei")
paths <- paste0(path1,path2,sep="")

myfiles <- vector("list",length(path2))
for(i in 1:length(paths)){
  fnames <- list.files(path=paths[i],pattern="*.csv")
  files <- paste0(paths[i],"\\",fnames,sep="")
  
  # First apply read.csv, then rbind
  myfiles[[i]] <- do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors=FALSE)))
}
remove(fnames,files,i)

#now only extract comments and combine as a matrix
file <- lapply(myfiles, function(x) x$content_coms)
file <- unlist(file)

```
* iconv(~, "CP949", "UTF-8)
"CP949"로 인코딩된 ~라는 데이터에 저장된 문자열을 "UTF-8"로 인코딩된 문자열로 변환
```{r}
file <- iconv(file, "CP949", "UTF-8") 
```

3. 전처리
* 한 post에 제보가 여러개 들어간 커뮤니티들이 있다. 이 글들을 별도의 vector item으로 구분하기 위해서
#를 이용한다. 서울대와 전대숲을 제외한 커뮤니티에서는 #로 각 제보에 태깅을 한다.(다행히도 서울대와 전대숲은 한 포스트에 하나의 제보만 올라간다)
이 점을 이용하여 #사이에 있는 nchar수가 일정 이상이 되는 경우에 #를 기준으로 substr을 이용하여 제보를 구분해 준다. 이는 제보 형식의 태깅 이외에 contents에 #가 포함된 경우를 피해주기 위함이다.
```{r}
sepfile <-unlist(sapply(X = file,FUN = function(x) strsplit(x,"#"),USE.NAMES = FALSE))

nchars <-unlist(sapply(sapply(X = file,FUN = function(x) strsplit(x,"#"),USE.NAMES = FALSE),nchar))

hist(x = nchars,breaks = c(200*0:40),col = c(3*0:40),labels = TRUE)

```
빈도를 그려보면 #을 기준으로 나누었을 때, 글자수가 10개 가장 많다.
다시 확인
```{r}
sum(nchars<10)
```

글자수가 10개 이하인 것은 사연이 아닌 것으로 간주하고 지움. 그리고 나머지로 나눈다.
간혹 글자수가 10개 이상인 것들 중에 sepfile[902] 같은 ...번째 속삭임 같은 것도 있지만 어차피 추후 걸러질 것이기 때문에 냅둔다.

```{r}
sepfile <- sepfile[nchars>10]
sepfile[902]
```

* 기타 이상한거 지움 
  주의점: replace를 " "로 해줘야 단어가 붙지 않고 떨어진다.
  
  <비교>
   - ""로 했을 때:
    $`카톡 읽씹하지마찔리는분들은 답장해주세요 `
    [1] "카톡"                   "읽씹하지마찔리는분들은"
    [3] "답장"  
   - " ":
    $`카톡 읽씹하지마    찔리는분들은 답장해주세요 `
    [1] "카톡"         "읽씹하지마"   "찔리는분들은" "답장"     
```{r}

sepfile <- str_replace_all(sepfile,"\n"," ")
sepfile <- str_replace_all(sepfile,"ㅋ[[:graph:]]*"," ") #ㅋ가 보이는 모든 문자를 지운다
sepfile <- str_replace_all(sepfile,"ㅎ[[:graph:]]*"," ")
sepfile <- str_replace_all(sepfile,"ㅜ[[:graph:]]*"," ")
sepfile <- str_replace_all(sepfile,"ㅠ[[:graph:]]*"," ")
sepfile <- str_replace_all(sepfile,"ㅗ[[:graph:]]*"," ")
sepfile <- str_replace_all(sepfile,"[:punct:]"," ")

#check
length(sepfile)

file <- sepfile
```
4. Building Corpus

* Extract Noun from txt

```{r}
txt <- file
remove(file)

txt_noun<-sapply(txt,extractNoun)
save(txt_noun,file = "txt_noun1.2.RData") #파일로 저장

```

* A vector source interprets each element of the vector x as a document.

```{r}
corpus <- Corpus(VectorSource(txt_noun)) 

```

* tm_map으로 다시한번 전처리 
```{r}
#corpus <- tm_map(corpus, removePunctuation)
#이미 부호 지우지 않았나?
#corpus <- tm_map(corpus, removeNumbers)
# 숫자를 지우는 이유?

```

* Document-Term Matrix
```{r}
uniTokenizer <- function(x){
  unlist(strsplit(as.character(x),useBytes = T), "[[:space:]]+")
}
control <- list(tokenize= uniTokenizer,
                removeNumbers = T,
                wordLengths=c(2,20),
                removePunctuation= T,
                stopwords = c("것"),
                weighting= function(x) weightTfIdf(x, T))
scanner <- function(x) strsplit(x," ") # alternative

tdm <- DocumentTermMatrix(corpus, list(tokenize=control))

tdm$dimnames$Terms <- iconv(tdm$dimnames$Terms,from = "CP949", to="UTF-8")


td <- removeSparseTerms(tdm, 0.99)
td$dimnames$Terms[1:10]
check<-which(rowSums(as.matrix(td))>1)
td<-(td[check,])
colTotal<-apply(td,1,sum)
which(colTotal<=0)
findFreqTerms(td, lowfreq=5)


TermFreq<-colSums(as.matrix(td))
TermFreq2 <-subset(TermFreq,TermFreq>100)
gframe<-data.frame(term=names(TermFreq2),freq=TermFreq2)
ggplot(data=gframe)+aes(x=term,y=freq)+geom_bar(stat="identity")+coord_flip()
wordcloud(names(TermFreq2),TermFreq2,random.color=TRUE,color=pal)

#LSA
LSA <-lsa(td,dim=4)
st <-LSA$tk
wd <- LSA$dk
strength <- LSA$sk

rot <- GPForth(wd, Tmat=diag(ncol(wd)), normalize=FALSE, eps=1e-5,
               maxit=10000, method="varimax",methodArgs=NULL)

cord<-st %*% diag(strength) %*% rot$Th #원래는 rot대신 wd를 쓰지만 여기서는 회전시킨 rot을 쓴다(김청택 교수님의 제안)
signs<-sign(colSums(rot$loadings))
cord<-cord %*% diag(signs)
text_lsa<-data.frame(cord=cord,txt=txt[check])

#######

t<-rot$loadings[,1]
tt<-abs(t)
terms<-names(tt)
wordcloud(terms,tt,scale=c(6,1),rot.per=0,max.words=50,color=pal)
```

