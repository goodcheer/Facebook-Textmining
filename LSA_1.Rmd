---
title: "페이스북 잠재의미분석"
output: html_notebook
---

1. Environment Setting & Packages loading
```{r}
setwd("C:\\Users\\kyuchul\\Documents\\Carrer\\RZIP")
packages <- c("Rfacebook","tm","lsa","wordcloud","ggplot2","KoNLP","GPArotation","cluster","RWeka","ROAuth","fpc","stringr","ape")
for(i in packages){
  library(i,character.only = T)
}
```

* 어떤 dictionary를 사용할 것인가 
```{r}
useSejongDic()
```
사전에 별도의 단어를 추가하기 위해서는 mergeUserDic(data.frame(c("item1","item2.."),c("품사")))

* Setting
```{r}
pdf.options(family="Korea1deb") #not to tear down the letters
options(java.parameters=c("-Xmx4g","-Dfile.encoding=UTF-8")) #to increse heap size of rjava
pal <- brewer.pal(9,"Set1")
options(mc.cores=1)
```

2. 대나무숲 파일 로딩
```{r}
files <- list.files(path="C:\\Users\\kyuchul\\Documents\\Carrer\\Rzip\\data_skku",pattern="*.csv")
# First apply read.csv, then rbind
myfiles = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))

file <- myfiles$content_coms

```
* iconv(~, "CP949", "UTF-8)
"CP949"로 인코딩된 ~라는 데이터에 저장된 문자열을 "UTF-8"로 인코딩된 문자열로 변환
```{r}
file <- iconv(file, "CP949", "UTF-8") 
```

3. 전처리

* file에서 내용이 시작하는 부분을 찾아냄 
  from : 처음으로 >\n이 등장하는 index + 2
  to : 처음으로 :]이 등장하는 index -1
  subtr로 그부분만 떼어냄
  
* 그리고 unname으로 file의 행이름을 지워줌 
* 기타 이상한거 지움 
  주의점: replace를 " "로 해줘야 단어가 붙지 않고 떨어진다.
  
  <비교>
   - ""로 했을 때:
    $`카톡 읽씹하지마찔리는분들은 답장해주세요 `
    [1] "카톡"                   "읽씹하지마찔리는분들은"
    [3] "답장"  
   - " ":
    $`카톡 읽씹하지마    찔리는분들은 답장해주세요 `
    [1] "카톡"         "읽씹하지마"   "찔리는분들은" "답장"     
```{r}
substrbody <- function(text){
  from <- gregexpr(pattern=">\n",text)[[1]][1] +2
  to <- if(gregexpr(pattern = ":]",text)[[1]][1]==-1){
    nchar(text)
  }else{
    gregexpr(":]",text)[[1]][1]-1
  }
  substr(text,from,to)
}

file <- sapply(file,substrbody)

file <- unname(file)
file <- str_replace_all(file,"\n"," ")
file <- str_replace_all(file,"ㅋ[[:graph:]]*"," ") #ㅋ가 보이는 모든 문자를 지운다
file <- str_replace_all(file,"ㅎ[[:graph:]]*"," ")
file <- str_replace_all(file,"ㅜ[[:graph:]]*"," ")
file <- str_replace_all(file,"ㅠ[[:graph:]]*"," ")
file <- str_replace_all(file,"ㅗ[[:graph:]]*"," ")
file <- str_replace_all(file,"[:punct:]"," ")

#check
length(file)
```
4. Building Corpus

* Extract Noun from txt

```{r}
txt <- file
remove(file)

txt_noun<-sapply(txt,extractNoun)
save(txt_noun,file = "txt_noun.RData") #파일로 저장

```

* A vector source interprets each element of the vector x as a document.

```{r}
corpus <- Corpus(VectorSource(txt_noun)) 

```

* tm_map으로 다시한번 전처리 
```{r}
#corpus <- tm_map(corpus, removePunctuation)
#이미 부호 지우지 않았나?
#corpus <- tm_map(corpus, removeNumbers)
# 숫자를 지우는 이유?

```

* Document-Term Matrix
```{r}
uniTokenizer <- function(x){
  unlist(strsplit(as.character(x),useBytes = T), "[[:space:]]+")
}
control <- list(tokenize= uniTokenizer,
                removeNumbers = T,
                wordLengths=c(2,20),
                removePunctuation= T,
                stopwords = c("것"),
                weighting= function(x) weightTfIdf(x, T))
scanner <- function(x) strsplit(x," ") # alternative

tdm <- DocumentTermMatrix(corpus, list(tokenize=control))

tdm$dimnames$Terms <- iconv(tdm$dimnames$Terms,from = "CP949", to="UTF-8")


td <- removeSparseTerms(tdm, 0.99)
td$dimnames$Terms[1:10]
check<-which(rowSums(as.matrix(td))>1)
td<-(td[check,])
colTotal<-apply(td,1,sum)
which(colTotal<=0)
findFreqTerms(td, lowfreq=5)


TermFreq<-colSums(as.matrix(td))
TermFreq2 <-subset(TermFreq,TermFreq>100)
gframe<-data.frame(term=names(TermFreq2),freq=TermFreq2)
ggplot(data=gframe)+aes(x=term,y=freq)+geom_bar(stat="identity")+coord_flip()
wordcloud(names(TermFreq2),TermFreq2,random.color=TRUE,color=pal)

#LSA
LSA <-lsa(td,dim=4)
st <-LSA$tk
wd <- LSA$dk
strength <- LSA$sk

rot <- GPForth(wd, Tmat=diag(ncol(wd)), normalize=FALSE, eps=1e-5,
               maxit=10000, method="varimax",methodArgs=NULL)

cord<-st %*% diag(strength) %*% rot$Th #원래는 rot대신 wd를 쓰지만 여기서는 회전시킨 rot을 쓴다(김청택 교수님의 제안)
signs<-sign(colSums(rot$loadings))
cord<-cord %*% diag(signs)
text_lsa<-data.frame(cord=cord,txt=txt[check])

#######

t<-rot$loadings[,3]
tt<-abs(t)
terms<-names(tt)
wordcloud(terms,tt,scale=c(6,1),rot.per=0,max.words=50,color=pal)
```

