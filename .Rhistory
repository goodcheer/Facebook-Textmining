function(x) read.csv(x, stringsAsFactors = FALSE,encoding = "UTF-8")))
files
myfiles = do.call(rbind,
lapply(files,
function(x) read.csv(x, stringsAsFactors = FALSE,fileEncoding = "UTF-8",)))
files <- list.files(pattern="*.csv")
myfiles = do.call(rbind,
lapply(files,
function(x) read.csv(x, stringsAsFactors = FALSE,fileEncoding = "UTF-8",)))
myfiles = do.call(rbind,
lapply(files,
function(x) read.csv(x, stringsAsFactors = FALSE,fileEncoding = "UTF-8",encoding="UTF-8")))
myfiles
myfiles = do.call(rbind,
lapply(files,
function(x) read.csv(x, stringsAsFactors = FALSE,fileEncoding = "UTF-8")))
myfiles = do.call(rbind,
lapply(files,
function(x) read.csv(x, stringsAsFactors = FALSE,encoding="UTF-8")))
files <- list.files(pattern="*.csv")
myfiles = do.call(rbind,
lapply(files,
function(x) read.csv(x, stringsAsFactors = FALSE)))
file <- myfiles$content_coms
file <- sapply(file,function(t){
from <- gregexpr(pattern=">\n",t)[[1]][1] +2
to <- gregexpr(pattern = ":]",t)[[1]][1] -1
substr(t,from,to)
})
file <- unname(file)
file <- str_replace_all(file,"\n"," ")
file <- str_replace_all(file,"ㅋ[[:graph:]]*"," ") #ㅋ가 보이는 모든 문자를 지운다
file <- str_replace_all(file,"ㅎ[[:graph:]]*"," ")
file <- str_replace_all(file,"ㅜ[[:graph:]]*"," ")
file <- str_replace_all(file,"ㅠ[[:graph:]]*"," ")
file <- str_replace_all(file,"ㅗ[[:graph:]]*"," ")
file <- str_replace_all(file,"[:punct:]"," ")
#Encoding(file) <- "UTF-8"
#check
length(file)
remove(myfiles)
remove(files)
remove(file)
files <- list.files(pattern="*.csv")
myfiles = do.call(rbind,
lapply(files,
function(x) read.csv(x, stringsAsFactors = FALSE,
fileEncoding = "cp949",
encoding = "UTF-8")))
file <- myfiles$content_coms
Encoding(file)
file[1]
sessionInfo()
Sys.getlocale()
Sys.setlocale("LC_CTYPE", "C")
Sys.getlocale()
Sys.setLocale("LC_ALL", "ko_KR.UTF-8")
출처: http://hreeman.tistory.com/174 [후뤼한잉여의 Bravo Hree Life!]
Sys.setLocale("LC_ALL", "ko_KR.UTF-8")
Sys.setlocale("LC_ALL", "ko_KR.UTF-8")
Sys.getlocale()
Sys.setlocale("LC_ALL", 'en_US.UTF-8')
Sys.setlocale("LC_ALL", "en_US.UTF-8")
Sys.getenv("language")
Sys.setenv(LANG="en")
Sys.getenv()
Sys.getenv(LANG)
Sys.getenv("LANG")
Sys.setlocale("LC_ALL", "en_US.UTF-8")
a
ei
file[1]
options(encoding = 'UTF-8')
file[1]
files <- list.files(pattern="*.csv")
myfiles = do.call(rbind,
lapply(files,
function(x) read.csv(x, stringsAsFactors = FALSE,
fileEncoding = "cp949")))
myfiles = do.call(rbind,
lapply(files,
function(x) read.csv(x, stringsAsFactors = FALSE)))
Sys.getenv()
setwd("C:\\Users\\kyuchul\\Documents\\Carrer\\RZIP")
packages <- c("Rfacebook","tm","lsa","wordcloud","ggplot2","KoNLP","GPArotation","cluster","RWeka","ROAuth","fpc","stringr","ape")
for(i in packages){
library(i,character.only = T)
}
Sys.getenv()
Sys.setenv(LANG = "en_US.UTF-8")
setwd("C:\\Users\\kyuchul\\Documents\\Carrer\\RZIP")
packages <- c("Rfacebook","tm","lsa","wordcloud","ggplot2","KoNLP","GPArotation","cluster","RWeka","ROAuth","fpc","stringr","ape")
for(i in packages){
library(i,character.only = T)
}
useSejongDic()
pdf.options(family="Korea1deb") #not to tear down the letters
options(java.parameters=c("-Xmx4g","-Dfile.encoding=UTF-8")) #to increse heap size of rjava
pal <- brewer.pal(9,"Set1")
options(mc.cores=1)
pdf.options(family="Korea1deb") #not to tear down the letters
options(java.parameters=c("-Xmx4g","-Dfile.encoding=UTF-8")) #to increse heap size of rjava
pal <- brewer.pal(9,"Set1")
options(mc.cores=1)
files <- list.files(pattern="*.csv")
myfiles = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
file <- myfiles$content_coms
file[1]
encoding(file)
Encoding(file)
iconv(file, "CP949", "UTF-8)
iconv(file, "CP949", "UTF-8")
file <- iconv(file, "CP949", "UTF-8")
encoding(file)
Encoding(file)
file[1]
file <- sapply(file,function(t){
from <- gregexpr(pattern=">\n",t)[[1]][1] +2
to <- gregexpr(pattern = ":]",t)[[1]][1] -1
substr(t,from,to)
})
file <- unname(file)
file <- str_replace_all(file,"\n"," ")
setwd("C:\\Users\\kyuchul\\Documents\\Carrer\\RZIP")
packages <- c("Rfacebook","tm","lsa","wordcloud","ggplot2","KoNLP","GPArotation","cluster","RWeka","ROAuth","fpc","stringr","ape")
for(i in packages){
library(i,character.only = T)
}
useSejongDic()
pdf.options(family="Korea1deb") #not to tear down the letters
options(java.parameters=c("-Xmx4g","-Dfile.encoding=UTF-8")) #to increse heap size of rjava
pal <- brewer.pal(9,"Set1")
options(mc.cores=1)
files <- list.files(pattern="*.csv")
# First apply read.csv, then rbind
myfiles = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
file <- myfiles$content_coms
Encoding(file)
file <- iconv(file, "CP949", "UTF-8")
Encoding(file)
file[1]
file <- sapply(file,function(t){
from <- gregexpr(pattern=">\n",t)[[1]][1] +2
to <- gregexpr(pattern = ":]",t)[[1]][1] -1
substr(t,from,to)
})
file[1]
Encoding(file)
file[14]
gregexpr(pattern = ":]",file[14])
if(gregexpr(pattern = ":]",file[14])[1]==-1) TRUE
if(gregexpr(pattern = ":]",file[14])[1]==-1) TRUE else FALSE
if(gregexpr(pattern = ":]",file[14])[1]==-1) FALSE
gregexpr(":]",file[14])
gregexpr(":]",file[14])[1]
gregexpr(":]",file[14])[1]
gregexpr(":]",file[14])[[1]]
gregexpr(":]",file[14])[[1]][1]
file <- sapply(file,function(t){
from <- gregexpr(pattern=">\n",t)[[1]][1] +2
to <- if(gregexpr(pattern = ":]",t)[[1]][1]==-1){
length(t)
}else{
gregexpr(":]",t)[[1]][1]-1
}
substr(t,from,to)
})
file[14]
toto <- function(t){
from <- gregexpr(pattern=">\n",t)[[1]][1] +2
to <- if(gregexpr(pattern = ":]",t)[[1]][1]==-1){
length(t)
}else{
gregexpr(":]",t)[[1]][1]-1
}
}
file[14]
name(file[14])
names(file[14])
t1 <- names(file[14])
t1
toto(t1)
toto
print(toto(t1))
toto <- function(t){
from <- gregexpr(pattern=">\\n",t)[[1]][1] +2
to <- if(gregexpr(pattern = ":]",t)[[1]][1]==-1){
length(t)
}else{
gregexpr(":]",t)[[1]][1]-1
}
}
toto <- function(t){
from <- gregexpr(pattern=">\\n",t)[[1]][1] +2
to <- if(gregexpr(pattern = ":]",t)[[1]][1]==-1){
length(t)
}else{
gregexpr(":]",t)[[1]][1]-1
}; print(from,to)
}
toto(t1)
toto <- function(t){
from <- gregexpr(pattern=">\\n",t)[[1]][1] +2
to <- if(gregexpr(pattern = ":]",t)[[1]][1]==-1){
length(t)
}else{
gregexpr(":]",t)[[1]][1]-1
}; print(from); print(to)
}
toto(t1)
length(t1)
length(t1)
nchar(t1)
toto <- function(t){
from <- gregexpr(pattern=">\\n",t)[[1]][1] +2
to <- if(gregexpr(pattern = ":]",t)[[1]][1]==-1){
nchar(t)
}else{
gregexpr(":]",t)[[1]][1]-1
}; print(from); print(to)
}
toto(t1)
substr(t1,34,380)
file <- sapply(file,function(t){
from <- gregexpr(pattern=">\n",t)[[1]][1] +2
to <- if(gregexpr(pattern = ":]",t)[[1]][1]==-1){
nchar(t)
}else{
gregexpr(":]",t)[[1]][1]-1
}
substr(t,from,to)
})
file[14]
toto
toto(t1)
substrbody <- function(text){
from <- gregexpr(pattern=">\n",t)[[1]][1] +2
to <- if(gregexpr(pattern = ":]",t)[[1]][1]==-1){
nchar(t)
}else{
gregexpr(":]",t)[[1]][1]-1
}
substr(t,from,to)
}
substrbody <- function(text){
from <- gregexpr(pattern=">\n",t)[[1]][1] +2
to <- if(gregexpr(pattern = ":]",t)[[1]][1]==-1){
nchar(t)
}else{
gregexpr(":]",t)[[1]][1]-1
}
substr(t,from,to)
}
substrbody <- function(text){
from <- gregexpr(pattern=">\n",text)[[1]][1] +2
to <- if(gregexpr(pattern = ":]",text)[[1]][1]==-1){
nchar(text)
}else{
gregexpr(":]",text)[[1]][1]-1
}
substr(text,from,to)
}
file[1]
file[14]
file <- myfiles$content_coms
file[1]
file <- iconv(file, "CP949", "UTF-8")
substrbody <- function(text){
from <- gregexpr(pattern=">\n",text)[[1]][1] +2
to <- if(gregexpr(pattern = ":]",text)[[1]][1]==-1){
nchar(text)
}else{
gregexpr(":]",text)[[1]][1]-1
}
substr(text,from,to)
}
file <- sapply(file,substrbody)
file[1]
file[2]
file[14]
file[30]
file[300]
file <- unname(file)
Encoding(file)
Encoding(file)=="unknown"
which(Encoding(file)=="unknown")
file[which(Encoding(file)=="unknown")]
unknowns<-which(Encoding(file)=="unknown")
myfiles$content_coms[unknowns]
file[1]
file[2]
file[14]
file <- str_replace_all(file,"\n"," ")
file <- str_replace_all(file,"ㅋ[[:graph:]]*"," ") #ㅋ가 보이는 모든 문자를 지운다
file <- str_replace_all(file,"ㅎ[[:graph:]]*"," ")
file <- str_replace_all(file,"ㅜ[[:graph:]]*"," ")
file <- str_replace_all(file,"ㅠ[[:graph:]]*"," ")
file <- str_replace_all(file,"ㅗ[[:graph:]]*"," ")
file <- str_replace_all(file,"[:punct:]"," ")
Encoding(file)
length(file)
txt <- file
remove(file)
txt_noun<-sapply(txt,extractNoun)
txt_noun[1]
Encoding(txt_noun)
Encoding(unlist(txt_noun))
txt_noun[1]
save(txt_noun,file = "txt_noun.RData") #파일로 저장
corpus <- Corpus(VectorSource(txt_noun))
corpus
corpus$1
inspect(corpus[1])
inspect(corpus[2])
inspect(corpus[14])
corpus <- tm_map(corpus, stripWhitespace) #불필요한 공백 제거
inpsect(corpus[4])
inspect(corpus[4])
corpus <- Corpus(VectorSource(txt_noun))
uniTokenizer <- function(x){
unlist(strsplit(as.character(x),useBytes = T), "[[:space:]]+")
}
control <- list(tokenize= uniTokenizer,
removeNumbers = T,
wordLengths=c(2,20),
removePunctuation= T,
stopwords = c("것"),
weighting= function(x) weightTfIdf(x, T))
scanner <- function(x) strsplit(x," ")
tdm <- DocumentTermMatrix(corpus, list(tokenize=control))
tdm
tdm$i
tdm$j
tdm$v
tdm$dimnames
tdm$dimnames$Terms
Encoding(tdm$dimnames$Terms)
Encoding(tdm$dimnames$Terms) ='UTF-8'
findFreqTerms(tdm,lowfreq = 2)
tdm <- DocumentTermMatrix(corpus, list(tokenize=control))
as.character("것")
tdm$dimnames$Terms <- iconv(tdm$dimnames$Terms,from = "CP949", to="UTF-8")
tdm$dimnames$Terms
Encoding(tdm$dimnames$Terms)
findFreqTerms(tdm,lowfreq = 2)
td <- removeSparseTerms(tdm, 0.99)
td$dimnames$Terms[1:10]
check<-which(rowSums(as.matrix(td))>1)
td<-(td[check,])
colTotal<-apply(td,1,sum)
which(colTotal<=0)
findFreqTerms(td, lowfreq=5)
TermFreq<-colSums(as.matrix(td))
TermFreq2 <-subset(TermFreq,TermFreq>4)
gframe<-data.frame(term=names(TermFreq2),freq=TermFreq2)
ggplot(data=gframe)+aes(x=term,y=freq)+geom_bar(stat="identity")+coord_flip()
wordcloud(names(TermFreq2),TermFreq2,random.color=TRUE,color=pal)
TermFreq<-colSums(as.matrix(td))
TermFreq2 <-subset(TermFreq,TermFreq>10)
gframe<-data.frame(term=names(TermFreq2),freq=TermFreq2)
ggplot(data=gframe)+aes(x=term,y=freq)+geom_bar(stat="identity")+coord_flip()
wordcloud(names(TermFreq2),TermFreq2,random.color=TRUE,color=pal)
TermFreq2 <-subset(TermFreq,TermFreq>30)
gframe<-data.frame(term=names(TermFreq2),freq=TermFreq2)
ggplot(data=gframe)+aes(x=term,y=freq)+geom_bar(stat="identity")+coord_flip()
wordcloud(names(TermFreq2),TermFreq2,random.color=TRUE,color=pal)
TermFreq2 <-subset(TermFreq,TermFreq>50)
gframe<-data.frame(term=names(TermFreq2),freq=TermFreq2)
ggplot(data=gframe)+aes(x=term,y=freq)+geom_bar(stat="identity")+coord_flip()
wordcloud(names(TermFreq2),TermFreq2,random.color=TRUE,color=pal)
TermFreq<-colSums(as.matrix(td))
TermFreq2 <-subset(TermFreq,TermFreq>100)
gframe<-data.frame(term=names(TermFreq2),freq=TermFreq2)
ggplot(data=gframe)+aes(x=term,y=freq)+geom_bar(stat="identity")+coord_flip()
wordcloud(names(TermFreq2),TermFreq2,random.color=TRUE,color=pal)
LSA <-lsa(td,dim=4)
st <-LSA$tk
wd <- LSA$dk
strength <- LSA$sk
rot <- GPForth(wd, Tmat=diag(ncol(wd)), normalize=FALSE, eps=1e-5,
maxit=10000, method="varimax",methodArgs=NULL)
cord<-st %*% diag(strength) %*% rot$Th #원래는 rot대신 wd를 쓰지만 여기서는 회전시킨 rot을 쓴다(김청택 교수님의 제안)
signs<-sign(colSums(rot$loadings))
cord<-cord %*% diag(signs)
text_lsa<-data.frame(cord=cord,txt=txt[check])
t<-rot$loadings[,3]
tt<-abs(t)
terms<-names(tt)
wordcloud(terms,tt,scale=c(6,1),rot.per=0,max.words=50,color=pal)
LSA
sum(LSA$sk)
td
td
td$nrow
td$ncol
Sys.getlocale()
stopwords("en")
td
corpus
inspect(corpus[1])
inspect(corpus[2])
TermFreq<-colSums(as.matrix(td))
TermFreq2 <-subset(TermFreq,TermFreq>4)
gframe<-data.frame(term=names(TermFreq2),freq=TermFreq2)
ggplot(data=gframe)+aes(x=term,y=freq)+geom_bar(stat="identity")+coord_flip()
wordcloud(names(TermFreq2),TermFreq2,random.color=TRUE,color=pal)
LSA <-lsa(td,dim=4)
LSA <-lsa(td,dim=4)
st <-LSA$tk
wd <- LSA$dk
strength <- LSA$sk
rot <- GPForth(wd, Tmat=diag(ncol(wd)), normalize=FALSE, eps=1e-5,
maxit=10000, method="varimax",methodArgs=NULL)
cord<-st %*% diag(strength) %*% rot$Th #원래는 rot대신 wd를 쓰지만 여기서는 회전시킨 rot을 쓴다(김청택 교수님의 제안)
signs<-sign(colSums(rot$loadings))
cord<-cord %*% diag(signs)
text_lsa<-data.frame(cord=cord,txt=txt[check])
text_lsa<-data.frame(cord=cord,txt=txt[check])
t<-rot$loadings[,3]
t<-rot$loadings[,3]
tt<-abs(t)
terms<-names(tt)
wordcloud(terms,tt,scale=c(6,1),rot.per=0,max.words=50,color=pal)
t<-rot$loadings[,2]
tt<-abs(t)
terms<-names(tt)
wordcloud(terms,tt,scale=c(6,1),rot.per=0,max.words=50,color=pal)
t<-rot$loadings[,1]
tt<-abs(t)
terms<-names(tt)
wordcloud(terms,tt,scale=c(6,1),rot.per=0,max.words=50,color=pal)
files
myfiles[1:10]
file[1]
file[,1]
View(file)
files
findFreqTerms(td, lowfreq=5)
control <- list(tokenize= uniTokenizer,
removeNumbers = T,
wordLengths=c(1,20),
removePunctuation= T,
stopwords = c("것"),
weighting= function(x) weightTfIdf(x, T))
scanner <- function(x) strsplit(x," ") # alternative
tdm <- DocumentTermMatrix(corpus, list(tokenize=control))
tdm$dimnames$Terms <- iconv(tdm$dimnames$Terms,from = "CP949", to="UTF-8")
td <- removeSparseTerms(tdm, 0.99)
td$dimnames$Terms[1:10]
check<-which(rowSums(as.matrix(td))>1)
td<-(td[check,])
colTotal<-apply(td,1,sum)
which(colTotal<=0)
findFreqTerms(td, lowfreq=5)
control <- list(tokenize= uniTokenizer,
removeNumbers = T,
#wordLengths=c(1,20),
removePunctuation= T,
stopwords = c("것"),
weighting= function(x) weightTfIdf(x, T))
tdm <- DocumentTermMatrix(corpus, list(tokenize=control))
tdm$dimnames$Terms <- iconv(tdm$dimnames$Terms,from = "CP949", to="UTF-8")
td <- removeSparseTerms(tdm, 0.99)
td$dimnames$Terms[1:10]
check<-which(rowSums(as.matrix(td))>1)
td<-(td[check,])
colTotal<-apply(td,1,sum)
findFreqTerms(td, lowfreq=2)
txt_noun$1
files <- list.files(pattern="*.csv")
myfiles = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
file <- myfiles$content_coms
file[1]
file <- iconv(file, "CP949", "UTF-8")
substrbody <- function(text){
from <- gregexpr(pattern=">\n",text)[[1]][1] +2
to <- if(gregexpr(pattern = ":]",text)[[1]][1]==-1){
nchar(text)
}else{
gregexpr(":]",text)[[1]][1]-1
}
substr(text,from,to)
}
file <- sapply(file,substrbody)
file <- unname(file)
file[1]
file[2]
file[3]
file <- str_replace_all(file,"\n"," ")
file[1]
file[1:3]
file <- str_replace_all(file,"ㅋ[[:graph:]]*"," ") #ㅋ가 보이는 모든 문자를 지운다
file[1:3]
file <- str_replace_all(file,"ㅎ[[:graph:]]*"," ")
file <- str_replace_all(file,"ㅜ[[:graph:]]*"," ")
file[1:3]
file <- str_replace_all(file,"ㅗ[[:graph:]]*"," ")
file <- str_replace_all(file,"[:punct:]"," ")
file[1:3]
txt <- file
txt_noun<-sapply(txt,extractNoun)
names(txt_noun)
txt_noun[1]
txt_noun[2]
txt_noun[3]
corpus <- Corpus(VectorSource(txt_noun))
inspect(corpus[1])
inspect(corpus[2])
inspect(corpus[3])
uniTokenizer <- function(x){
unlist(strsplit(as.character(x),useBytes = T), "[[:space:]]+")
}
